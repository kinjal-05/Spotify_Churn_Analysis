# -*- coding: utf-8 -*-
"""Spotify_Churn_Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dkzWRirpVj-8VYNVQ690lFkmupz8lIyL
"""

from google.colab import files
import pandas as pd

# Upload file
uploaded = files.upload()

# Get the file name (first uploaded file)
filename = list(uploaded.keys())[0]

# Read CSV into pandas DataFrame
df = pd.read_csv(filename)

# Show first 5 rows
df.head()

# Show all column names
print(df.columns.tolist())

# Total rows
print("Total rows:", len(df))

# Or
print("Total rows:", df.shape[0])

# Count of each unique value in Gender
print(df['gender'].value_counts())

# Count including NaN values
print(df['gender'].value_counts(dropna=False))

# Basic statistics (useful for categorical data)
print(df['gender'].describe())

import matplotlib.pyplot as plt

# Count of each gender
gender_counts = df['gender'].value_counts()

# Plot bar chart
plt.figure(figsize=(6,4))
gender_counts.plot(kind='bar', color=['skyblue', 'lightcoral'])

plt.title('Gender Distribution')
plt.xlabel('Gender')
plt.ylabel('Count')
plt.show()

# Count NaN values in Gender column
nan_count = df['gender'].isna().sum()
print("Number of NaN values in Gender column:", nan_count)

# Check if any null values exist in Gender column
has_nulls = df['gender'].isnull().any()
print("Any null values in Gender column? ", has_nulls)

print("Total null values in Gender column:", df['gender'].isnull().sum())

# Summary statistics for Age column
print(df['age'].describe())

import matplotlib.pyplot as plt

# Histogram for Age distribution
plt.figure(figsize=(6,4))
plt.hist(df['age'].dropna(), bins=20, color='skyblue', edgecolor='black')
plt.title('Age Distribution')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.show()

# Boxplot for Age
plt.figure(figsize=(4,6))
plt.boxplot(df['age'].dropna(), vert=True)
plt.title('Age Boxplot')
plt.ylabel('Age')
plt.show()

# Check if Age column has any null values
has_nulls = df['age'].isnull().any()
print("Any null values in Age column?", has_nulls)

# Count how many null values
null_count = df['age'].isnull().sum()
print("Total null values in Age column:", null_count)

# Summary statistics for Country column
print(df['country'].describe())

import matplotlib.pyplot as plt

# Count of each country
country_counts = df['country'].value_counts()

# Bar chart
plt.figure(figsize=(10,5))
country_counts.plot(kind='bar', color='lightgreen', edgecolor='black')

plt.title('Country Distribution')
plt.xlabel('Country')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.show()

# Check if any null values exist
has_nulls = df['country'].isnull().any()
print("Any null or NaN values in Country column?", has_nulls)

# Count total null values
null_count = df['country'].isna().sum()
print("Total null or NaN values in Country column:", null_count)

# Summary of subscription_type column
print(df['subscription_type'].describe())

# Count of each type
print("\nCounts of each subscription type:")
print(df['subscription_type'].value_counts())

# Count including NaN values
print("\nCounts including NaN values:")
print(df['subscription_type'].value_counts(dropna=False))

import matplotlib.pyplot as plt

# Count of each subscription type
subscription_counts = df['subscription_type'].value_counts()

# Bar chart
plt.figure(figsize=(8,5))
subscription_counts.plot(kind='bar', color='skyblue', edgecolor='black')

plt.title('Subscription Type Distribution')
plt.xlabel('Subscription Type')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.show()

# Check if any NaN values exist
has_nan = df['subscription_type'].isnull().any()
print("Any NaN values in 'subscription_type' column?", has_nan)

# Count total NaN values
nan_count = df['subscription_type'].isna().sum()
print("Total NaN values in 'subscription_type' column:", nan_count)

# Summary statistics for listening_time column
print(df['listening_time'].describe())

# Count of missing values
missing_count = df['listening_time'].isnull().sum()
print("\nTotal missing values in listening_time:", missing_count)

import matplotlib.pyplot as plt

# Histogram for listening_time
plt.figure(figsize=(6,4))
plt.hist(df['listening_time'].dropna(), bins=20, color='lightblue', edgecolor='black')
plt.title('Listening Time Distribution')
plt.xlabel('Listening Time')
plt.ylabel('Frequency')
plt.show()

# Boxplot for listening_time
plt.figure(figsize=(4,6))
plt.boxplot(df['listening_time'].dropna(), vert=True)
plt.title('Listening Time Boxplot')
plt.ylabel('Listening Time')
plt.show()

# Summary statistics for songs_played_per_day column
print(df['songs_played_per_day'].describe())

# Count of missing values
missing_count = df['songs_played_per_day'].isnull().sum()
print("\nTotal missing values in songs_played_per_day:", missing_count)

import matplotlib.pyplot as plt

# Histogram for songs_played_per_day
plt.figure(figsize=(6,4))
plt.hist(df['songs_played_per_day'].dropna(), bins=20, color='lightcoral', edgecolor='black')
plt.title('Songs Played Per Day Distribution')
plt.xlabel('Songs Played Per Day')
plt.ylabel('Frequency')
plt.show()

# Boxplot for songs_played_per_day
plt.figure(figsize=(4,6))
plt.boxplot(df['songs_played_per_day'].dropna(), vert=True)
plt.title('Songs Played Per Day Boxplot')
plt.ylabel('Songs Played Per Day')
plt.show()

# Summary statistics for skip_rate column
print(df['skip_rate'].describe())

# Count of missing values
missing_count = df['skip_rate'].isnull().sum()
print("\nTotal missing values in skip_rate:", missing_count)

import matplotlib.pyplot as plt

# Histogram for skip_rate
plt.figure(figsize=(6,4))
plt.hist(df['skip_rate'].dropna(), bins=20, color='lightgreen', edgecolor='black')
plt.title('Skip Rate Distribution')
plt.xlabel('Skip Rate')
plt.ylabel('Frequency')
plt.show()

# Boxplot for skip_rate
plt.figure(figsize=(4,6))
plt.boxplot(df['skip_rate'].dropna(), vert=True)
plt.title('Skip Rate Boxplot')
plt.ylabel('Skip Rate')
plt.show()

# Summary statistics for device_type column
print(df['device_type'].describe())

# Count of each device type
print("\nCounts of each device type:")
print(df['device_type'].value_counts())

# Count including NaN values
print("\nCounts including NaN values:")
print(df['device_type'].value_counts(dropna=False))

import matplotlib.pyplot as plt

# Count of each device type
device_counts = df['device_type'].value_counts()

# Bar chart
plt.figure(figsize=(8,5))
device_counts.plot(kind='bar', color='lightblue', edgecolor='black')

plt.title('Device Type Distribution')
plt.xlabel('Device Type')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.show()

# Check if any null or NaN values exist
has_nan = df['device_type'].isnull().any()
print("Any null or NaN values in 'device_type' column?", has_nan)

# Count total null or NaN values
nan_count = df['device_type'].isna().sum()
print("Total null or NaN values in 'device_type' column:", nan_count)

# Summary statistics for device_type column
print(df['ads_listened_per_week'].describe())

# Count of each device type
print("\nCounts of each device type:")
print(df['ads_listened_per_week'].value_counts())

# Count including NaN values
print("\nCounts including NaN values:")
print(df['ads_listened_per_week'].value_counts(dropna=False))

import matplotlib.pyplot as plt

# Histogram for ads_listened_per_week
plt.figure(figsize=(6,4))
plt.hist(df['ads_listened_per_week'].dropna(), bins=20, color='lightcoral', edgecolor='black')
plt.title('Ads Listened Per Week Distribution')
plt.xlabel('Ads Listened Per Week')
plt.ylabel('Frequency')
plt.show()

# Boxplot for ads_listened_per_week
plt.figure(figsize=(4,6))
plt.boxplot(df['ads_listened_per_week'].dropna(), vert=True)
plt.title('Ads Listened Per Week Boxplot')
plt.ylabel('Ads Listened Per Week')
plt.show()

# Summary statistics for offline_listening column
print(df['offline_listening'].describe())

# Count of each category
print("\nCounts of each category:")
print(df['offline_listening'].value_counts())

# Count including NaN values
print("\nCounts including NaN values:")
print(df['offline_listening'].value_counts(dropna=False))

# Check if any null or NaN values exist
has_nan = df['offline_listening'].isnull().any()
print("Any null or NaN values in 'offline_listening' column?", has_nan)

# Count total null or NaN values
nan_count = df['offline_listening'].isnull().sum()
print("Total null or NaN values in 'offline_listening' column:", nan_count)

# Summary statistics for is_churned column
print(df['is_churned'].describe())

# Count of each category
print("\nCounts of each category:")
print(df['is_churned'].value_counts())

# Count including NaN values
print("\nCounts including NaN values:")
print(df['is_churned'].value_counts(dropna=False))

import matplotlib.pyplot as plt

# Count of each category
churn_counts = df['is_churned'].value_counts()

# Bar chart
plt.figure(figsize=(6,4))
churn_counts.plot(kind='bar', color=['lightblue', 'salmon'], edgecolor='black')

plt.title('Churned Users Distribution')
plt.xlabel('Is Churned')
plt.ylabel('Count')
plt.xticks(rotation=0)
plt.show()

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# If categorical columns exist, encode them using label encoding
from sklearn.preprocessing import LabelEncoder

df_encoded = df.copy()
categorical_cols = ['gender', 'country', 'subscription_type', 'device_type', 'offline_listening', 'is_churned']

for col in categorical_cols:
    df_encoded[col] = LabelEncoder().fit_transform(df_encoded[col].astype(str))

# Compute correlation matrix
corr = df_encoded.corr()

# Plot heatmap
plt.figure(figsize=(12,8))
sns.heatmap(corr, annot=True, fmt=".2f", cmap='coolwarm', cbar=True)
plt.title('Correlation Heatmap of All Columns')
plt.show()

import pandas as pd
from sklearn.preprocessing import OneHotEncoder

# Create a copy of the dataframe
df_encoded = df.copy()

# Initialize OneHotEncoder
encoder = OneHotEncoder(drop=None)  # drop=None keeps all categories

# Fit and transform the 'gender' column
gender_encoded = encoder.fit_transform(df_encoded[['gender']])

# Get column names for the one-hot encoded columns
gender_columns = encoder.get_feature_names_out(['gender'])

# Convert to DataFrame
gender_df = pd.DataFrame(gender_encoded.toarray(), columns=gender_columns, index=df_encoded.index)

# Concatenate with original dataframe and drop the original 'gender' column
df_encoded = pd.concat([df_encoded, gender_df], axis=1)
df_encoded = df_encoded.drop('gender', axis=1)

# Check the result
df_encoded.head()

import pandas as pd
from sklearn.preprocessing import OneHotEncoder

# Create a copy of the dataframe
df_encoded1 = df_encoded.copy()

# Convert 'country' column to categorical type
df_encoded1['country'] = df_encoded1['country'].astype('category')

# Initialize OneHotEncoder
encoder = OneHotEncoder(drop=None)

# Fit and transform the 'country' column
country_encoded = encoder.fit_transform(df_encoded1[['country']])

# Get column names for the one-hot encoded columns
country_columns = encoder.get_feature_names_out(['country'])

# Convert to DataFrame with integer values
country_df = pd.DataFrame(country_encoded.toarray().astype(int), columns=country_columns, index=df_encoded1.index)

# Concatenate with original dataframe and drop the original 'country' column
df_encoded1 = pd.concat([df_encoded1, country_df], axis=1)
df_encoded1 = df_encoded1.drop('country', axis=1)

# Check the result
df_encoded1.head()

import pandas as pd
from sklearn.preprocessing import OneHotEncoder

# Create a copy of the dataframe
df_encoded2 = df_encoded1.copy()

# Initialize OneHotEncoder
encoder = OneHotEncoder(drop=None)

# Fit and transform the 'subscription_type' column
subscription_encoded = encoder.fit_transform(df_encoded2[['subscription_type']])

# Get column names for the one-hot encoded columns
subscription_columns = encoder.get_feature_names_out(['subscription_type'])

# Convert to DataFrame with integer values
subscription_df = pd.DataFrame(subscription_encoded.toarray().astype(int),
                               columns=subscription_columns,
                               index=df_encoded2.index)

# Concatenate with original dataframe and drop the original 'subscription_type' column
df_encoded2 = pd.concat([df_encoded2, subscription_df], axis=1)
df_encoded2 = df_encoded2.drop('subscription_type', axis=1)

# Check the result
df_encoded2.head()

# Convert gender columns to integer
df_encoded2[['gender_Male', 'gender_Female', 'gender_Other']] = df_encoded2[['gender_Male', 'gender_Female', 'gender_Other']].astype(int)

# Check the result
df_encoded2[['gender_Male', 'gender_Female', 'gender_Other']].head()

df_encoded2.head()

import pandas as pd
from sklearn.preprocessing import OneHotEncoder

# Create a copy of the dataframe
df_encoded3 = df_encoded2.copy()

# Initialize OneHotEncoder
encoder = OneHotEncoder(drop=None)

# Fit and transform the 'device_type' column
device_encoded = encoder.fit_transform(df_encoded3[['device_type']])

# Get column names for the one-hot encoded columns
device_columns = encoder.get_feature_names_out(['device_type'])

# Convert to DataFrame with integer values
device_df = pd.DataFrame(device_encoded.toarray().astype(int),
                         columns=device_columns,
                         index=df_encoded3.index) # Use df_encoded3.index here

# Concatenate with original dataframe and drop the original 'device_type' column
df_encoded3 = pd.concat([df_encoded3, device_df], axis=1)
df_encoded3 = df_encoded3.drop('device_type', axis=1)

# Check the result
df_encoded3.head()

df=df_encoded3
df.head()

# Save DataFrame to CSV
df.to_csv('spotify.csv', index=False)

# If using df_encoded after preprocessing:
# df_encoded.to_csv('spotify.csv', index=False)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Assume your DataFrame is df (already preprocessed with one-hot encoding)

# Define features and target
X = df.drop(columns=['user_id', 'is_churned'])
y = df['is_churned']

# Split into train and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Initialize Random Forest Classifier with all hyperparameters
rf_model = RandomForestClassifier(
    n_estimators=200,        # number of trees
    max_depth=10,            # maximum depth of each tree
    min_samples_split=5,     # min samples required to split a node
    min_samples_leaf=2,      # min samples required at a leaf node
    max_features='sqrt',     # number of features to consider at each split ('sqrt', 'log2', or int)
    random_state=42
)

# Train the model
rf_model.fit(X_train, y_train)

# Predict on test set
y_pred = rf_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
report = classification_report(y_test, y_pred)

print("Accuracy:", accuracy)
print("\nConfusion Matrix:\n", conf_matrix)
print("\nClassification Report:\n", report)

# Optional: feature importance
feature_importances = pd.DataFrame({
    'feature': X.columns,
    'importance': rf_model.feature_importances_
}).sort_values(by='importance', ascending=False)

print("\nFeature Importances:\n", feature_importances)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Assume your DataFrame is df (already preprocessed with one-hot encoding)

# Define features and target
X = df.drop(columns=['user_id', 'is_churned'])
y = df['is_churned']

# Split into train and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Initialize Random Forest Classifier with all hyperparameters
rf_model = RandomForestClassifier(
    n_estimators=200,
    max_depth=10,
    min_samples_split=5,
    min_samples_leaf=2,
    max_features='sqrt',
    class_weight='balanced',  # balances class weights
    random_state=42
)

# Train the model
rf_model.fit(X_train, y_train)

# Predict on test set
y_pred = rf_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
report = classification_report(y_test, y_pred)

print("Accuracy:", accuracy)
print("\nConfusion Matrix:\n", conf_matrix)
print("\nClassification Report:\n", report)

# Optional: feature importance
feature_importances = pd.DataFrame({
    'feature': X.columns,
    'importance': rf_model.feature_importances_
}).sort_values(by='importance', ascending=False)

print("\nFeature Importances:\n", feature_importances)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import xgboost as xgb

# Assume your data is in a DataFrame called df
# Features and target
X = df.drop('user_id', axis=1)
X = X.drop('is_churned', axis=1)
y = df['is_churned']

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Handle class imbalance
# Calculate scale_pos_weight = #negatives / #positives
scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()

# Initialize XGBoost classifier
model = xgb.XGBClassifier(
    n_estimators=200,        # Number of trees
    max_depth=6,             # Max depth of each tree
    learning_rate=0.1,       # Step size shrinkage
    subsample=0.8,           # Random subsample ratio
    colsample_bytree=0.8,    # Random features ratio per tree
    scale_pos_weight=scale_pos_weight,  # Handle imbalance
    random_state=42,
    use_label_encoder=False,
    eval_metric='logloss'
)

# Train the model
model.fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)

# Evaluate
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import xgboost as xgb

# Assume your data is in a DataFrame called df
# Features and target
X = df.drop('user_id', axis=1)
X = X.drop('is_churned', axis=1)
X = X.drop('is_churned', axis=1)
y = df['is_churned']

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Handle class imbalance
# Calculate scale_pos_weight = #negatives / #positives
scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()

# Initialize XGBoost classifier
model = xgb.XGBClassifier(
    n_estimators=200,        # Number of trees
    max_depth=6,             # Max depth of each tree
    learning_rate=0.1,       # Step size shrinkage
    subsample=0.8,           # Random subsample ratio
    colsample_bytree=0.8,    # Random features ratio per tree
    scale_pos_weight=scale_pos_weight,  # Handle imbalance
    random_state=42,
    use_label_encoder=False,
    eval_metric='logloss'
)

# Train the model
model.fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)

# Evaluate
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import xgboost as xgb

# Columns to drop (low importance)
low_importance_cols = [
    'device_type_Mobile',
    'gender_Female',
    'subscription_type_Premium',
    'subscription_type_Student',
    'subscription_type_Family',
    'country_US',
    'country_UK',
    'country_IN',
    'country_AU',
    'country_PK',
    'country_DE',
    'country_CA',
    'country_FR',
    'offline_listening',
    'subscription_type_Free'
]

# Features and target
X = df.drop(columns=['user_id', 'is_churned'] + low_importance_cols)
y = df['is_churned']

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Handle class imbalance
scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()

# Initialize XGBoost classifier
model = xgb.XGBClassifier(
    n_estimators=200,
    max_depth=6,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    scale_pos_weight=scale_pos_weight,
    random_state=42,
    use_label_encoder=False,
    eval_metric='logloss'
)

# Train the model
model.fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)

# Evaluate
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

df.head()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import xgboost as xgb

# Columns to drop (low importance)
low_importance_cols = [
    'device_type_Mobile',
    'gender_Female',
    'subscription_type_Premium',
    'subscription_type_Student',
    'subscription_type_Family',
    'country_US',
    'country_UK',
    'country_IN',
    'country_AU',
    'country_PK',
    'country_DE',
    'country_CA',
    'country_FR',
    'offline_listening',
    'subscription_type_Free'
]

# Features and target
X = df.drop(columns=['user_id', 'is_churned'] + low_importance_cols)
y = df['is_churned']

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Handle class imbalance
scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()

# Initialize XGBoost classifier
model = xgb.XGBClassifier(
    n_estimators=200,
    max_depth=6,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    scale_pos_weight=scale_pos_weight,
    random_state=42,
    use_label_encoder=False,
    eval_metric='logloss'
)

# Train the model
model.fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)

# Evaluate
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import xgboost as xgb

# Columns to drop (low importance)
low_importance_cols = [
    'ads_listened_per_week',
    'device_type_Web',
    'device_type_Desktop',
    'gender_Other',
    'gender_Male',
    'device_type_Mobile',
    'gender_Female',
    'subscription_type_Premium',
    'subscription_type_Student',
    'subscription_type_Family',
    'country_US',
    'country_UK',
    'country_IN',
    'country_AU',
    'country_PK',
    'country_DE',
    'country_CA',
    'country_FR',
    'offline_listening',
    'subscription_type_Free'
]

# Features and target
X = df.drop(columns=['user_id', 'is_churned'] + low_importance_cols)
y = df['is_churned']

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Handle class imbalance
scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()

# Initialize XGBoost classifier
model = xgb.XGBClassifier(
    n_estimators=200,
    max_depth=6,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    scale_pos_weight=scale_pos_weight,
    random_state=42,
    use_label_encoder=False,
    eval_metric='logloss'
)

# Train the model
model.fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)

# Evaluate
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

import matplotlib.pyplot as plt
import seaborn as sns

# Assuming df has 'age' and 'is_churned' columns

# Option 1: Boxplot to see age distribution by churn
plt.figure(figsize=(8,6))
sns.boxplot(x='is_churned', y='age', data=df)
plt.title('Age distribution by churn status')
plt.xlabel('Is Churned')
plt.ylabel('Age')
plt.show()

# Option 2: Bar plot of churn rate by age
churn_rate_by_age = df.groupby('age')['is_churned'].mean().reset_index()
plt.figure(figsize=(10,6))
sns.barplot(x='age', y='is_churned', data=churn_rate_by_age, palette='viridis')
plt.title('Churn rate by age')
plt.xlabel('Age')
plt.ylabel('Churn Rate')
plt.xticks(rotation=45)
plt.show()

import matplotlib.pyplot as plt
import pandas as pd

# Calculate churn rate by age
churn_rate_by_age = df.groupby('age')['is_churned'].mean().reset_index()

# Plot line graph
plt.figure(figsize=(10,6))
plt.plot(churn_rate_by_age['age'], churn_rate_by_age['is_churned'], marker='o', linestyle='-', color='blue')
plt.title('Churn Rate by Age')
plt.xlabel('Age')
plt.ylabel('Churn Rate')
plt.grid(True)
plt.xticks(rotation=45)
plt.show()

import matplotlib.pyplot as plt
import pandas as pd

# Count of users by age and churn status
age_churn_counts = df.groupby(['age', 'is_churned']).size().unstack(fill_value=0)

# Plot line graph
plt.figure(figsize=(10,6))
plt.plot(age_churn_counts.index, age_churn_counts[0], marker='o', linestyle='-', label='Not Churned', color='green')
plt.plot(age_churn_counts.index, age_churn_counts[1], marker='o', linestyle='-', label='Churned', color='red')

plt.title('Number of Users by Age and Churn Status')
plt.xlabel('Age')
plt.ylabel('Number of Users')
plt.legend()
plt.grid(True)
plt.xticks(rotation=45)
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Add a small jitter to y to avoid overlap
y_jitter = df['is_churned'] + np.random.uniform(-0.05, 0.05, size=df.shape[0])

plt.figure(figsize=(10,6))
plt.scatter(df['age'], y_jitter, c=df['is_churned'], cmap='bwr', alpha=0.6)
plt.title('Scatter plot of Age vs Churn Status')
plt.xlabel('Age')
plt.ylabel('Is Churned (jittered)')
plt.yticks([0,1], ['Not Churned', 'Churned'])
plt.grid(True)
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Add jitter to y so points don't overlap
y_jitter = df['is_churned'] + np.random.uniform(-0.05, 0.05, size=df.shape[0])

plt.figure(figsize=(10,6))
plt.scatter(df['listening_time'], y_jitter, c=df['is_churned'], cmap='bwr', alpha=0.6)
plt.title('Scatter plot of Listening Rate vs Churn Status')
plt.xlabel('Listening Time')
plt.ylabel('Is Churned (jittered)')
plt.yticks([0,1], ['Not Churned', 'Churned'])
plt.grid(True)
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Add jitter to y to avoid overlap
y_jitter = df['is_churned'] + np.random.uniform(-0.05, 0.05, size=df.shape[0])

plt.figure(figsize=(10,6))
plt.scatter(df['songs_played_per_day'], y_jitter, c=df['is_churned'], cmap='bwr', alpha=0.6)
plt.title('Scatter Plot of Songs Played per Day vs Churn Status')
plt.xlabel('Songs Played per Day')
plt.ylabel('Is Churned (jittered)')
plt.yticks([0,1], ['Not Churned', 'Churned'])
plt.grid(True)
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Add small jitter to y so points don't overlap
y_jitter = df['is_churned'] + np.random.uniform(-0.05, 0.05, size=df.shape[0])

plt.figure(figsize=(10,6))
plt.scatter(df['skip_rate'], y_jitter, c=df['is_churned'], cmap='bwr', alpha=0.6)
plt.title('Scatter Plot of Skip Rate vs Churn Status')
plt.xlabel('Skip Rate')
plt.ylabel('Is Churned (jittered)')
plt.yticks([0,1], ['Not Churned', 'Churned'])
plt.grid(True)
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Select only the features of interest plus target
features = ['listening_time', 'songs_played_per_day', 'skip_rate', 'age']
df_plot = df[features + ['is_churned']]

# Melt the DataFrame for seaborn boxplot
df_melt = df_plot.melt(id_vars='is_churned', value_vars=features,
                       var_name='Feature', value_name='Value')

plt.figure(figsize=(12,6))
sns.boxplot(x='Feature', y='Value', hue='is_churned', data=df_melt, palette='Set1')
plt.title('Relationship of Features with Churn Status')
plt.xlabel('Feature')
plt.ylabel('Value')
plt.legend(title='Is Churned', loc='upper right', labels=['Not Churned', 'Churned'])
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Select only the features of interest plus target
features = ['listening_time', 'songs_played_per_day', 'skip_rate', 'age', 'is_churned']
df_plot = df[features]

# Pairplot to visualize feature relationships colored by churn
sns.pairplot(df_plot, hue='is_churned', palette='Set1', diag_kind='kde', plot_kws={'alpha':0.5})
plt.suptitle('Pairwise Feature Relationships by Churn Status', y=1.02)
plt.show()

import pandas as pd
from sklearn.cluster import KMeans
from sklearn.metrics import confusion_matrix, accuracy_score
import matplotlib.pyplot as plt

# Select age as input
X = df[['age']].values  # 2D array for KMeans

# Initialize KMeans for k=2
kmeans = KMeans(n_clusters=2, random_state=42)
kmeans.fit(X)

# Get cluster labels
clusters = kmeans.labels_

# Optional: Compare clusters with actual churn
# Note: cluster labels are arbitrary (0 or 1), may need to flip
from sklearn.metrics import accuracy_score

# Try both label orders to see best match
accuracy1 = accuracy_score(df['is_churned'], clusters)
accuracy2 = accuracy_score(df['is_churned'], 1 - clusters)
best_accuracy = max(accuracy1, accuracy2)

print("Best alignment accuracy with is_churned:", best_accuracy)

# Confusion Matrix
cm = confusion_matrix(df['is_churned'], clusters if accuracy1>accuracy2 else 1-clusters)
print("Confusion Matrix:\n", cm)

# Plot clusters
plt.figure(figsize=(8,6))
plt.scatter(X, df['is_churned'], c=clusters, cmap='viridis', alpha=0.6)
plt.xlabel('Age')
plt.ylabel('Is Churned')
plt.title('K-Means Clustering (k=2) on Age')
plt.show()

import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns

# Features and target
features = ['listening_time', 'songs_played_per_day', 'skip_rate', 'age']
X = df[features].values
y = df['is_churned'].values

# Standardize features (important for K-Means)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Initialize K-Means for k=2
kmeans = KMeans(n_clusters=2, random_state=42)
kmeans.fit(X_scaled)

# Get cluster labels
clusters = kmeans.labels_

# Check alignment with actual churn (labels are arbitrary, so check both)
accuracy1 = accuracy_score(y, clusters)
accuracy2 = accuracy_score(y, 1 - clusters)
best_accuracy = max(accuracy1, accuracy2)

print("Best alignment accuracy with is_churned:", best_accuracy)

# Confusion Matrix
cm = confusion_matrix(y, clusters if accuracy1>accuracy2 else 1-clusters)
print("Confusion Matrix:\n", cm)

# Visualize clusters using first two principal features (for simplicity)
plt.figure(figsize=(10,6))
sns.scatterplot(x=X_scaled[:,0], y=X_scaled[:,1], hue=clusters, palette='Set1', alpha=0.6)
plt.xlabel(features[0])
plt.ylabel(features[1])
plt.title('K-Means Clustering (k=2) on Multiple Features')
plt.show()

import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns

# Features and target
features = ['listening_time', 'songs_played_per_day', 'skip_rate', 'age']
X = df[features].values
y = df['is_churned'].values

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Agglomerative clustering
agg_cluster = AgglomerativeClustering(n_clusters=2)
clusters = agg_cluster.fit_predict(X_scaled)

# Check alignment with actual churn
accuracy1 = accuracy_score(y, clusters)
accuracy2 = accuracy_score(y, 1 - clusters)
best_accuracy = max(accuracy1, accuracy2)

print("Best alignment accuracy with is_churned:", best_accuracy)

# Confusion matrix
cm = confusion_matrix(y, clusters if accuracy1>accuracy2 else 1-clusters)
print("Confusion Matrix:\n", cm)

# Visualize clusters using first two principal features
plt.figure(figsize=(10,6))
sns.scatterplot(x=X_scaled[:,0], y=X_scaled[:,1], hue=clusters, palette='Set1', alpha=0.6)
plt.xlabel(features[0])
plt.ylabel(features[1])
plt.title('Agglomerative Clustering (k=2) on Multiple Features')
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
import matplotlib.pyplot as plt
import numpy as np

# Features and target
X = df[['age']].values  # 2D array
y = df['is_churned'].values

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Scale features (important for SVM)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize SVM (linear kernel for simplicity)
svm_model = SVC(kernel='linear', class_weight='balanced', random_state=42)
svm_model.fit(X_train_scaled, y_train)

# Predict
y_pred = svm_model.predict(X_test_scaled)

# Evaluate
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# Optional: visualize decision boundary
plt.figure(figsize=(8,6))
# Scatter plot of age vs churn
y_jitter = y_test + np.random.uniform(-0.05, 0.05, size=y_test.shape)
plt.scatter(X_test_scaled, y_jitter, c=y_test, cmap='bwr', alpha=0.6)

# Plot decision boundary
x_vals = np.linspace(X_test_scaled.min()-1, X_test_scaled.max()+1, 100)
w = svm_model.coef_[0]
b = svm_model.intercept_[0]
y_decision = -(w[0]*x_vals + b)/1  # since only one feature
plt.plot(x_vals, y_decision, 'k--', label='Decision boundary')

plt.xlabel('Age (scaled)')
plt.ylabel('Is Churned (jittered)')
plt.title('SVM Decision Boundary on Age')
plt.yticks([0,1], ['Not Churned', 'Churned'])
plt.legend()
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
import matplotlib.pyplot as plt
import numpy as np

# Features and target
X = df[['age']].values  # 2D array
y = df['is_churned'].values

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize Logistic Regression
logreg = LogisticRegression(class_weight='balanced', random_state=42)
logreg.fit(X_train_scaled, y_train)

# Predict
y_pred = logreg.predict(X_test_scaled)

# Evaluate
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# Optional: visualize probability curve
plt.figure(figsize=(8,6))
plt.scatter(X_test_scaled, y_test + np.random.uniform(-0.05,0.05,size=y_test.shape),
            c=y_test, cmap='bwr', alpha=0.6, label='Actual')

# Logistic regression curve
x_vals = np.linspace(X_test_scaled.min()-1, X_test_scaled.max()+1, 300).reshape(-1,1)
y_prob = logreg.predict_proba(x_vals)[:,1]
plt.plot(x_vals, y_prob, 'k-', label='Predicted Probability')

plt.xlabel('Age (scaled)')
plt.ylabel('Is Churned')
plt.title('Logistic Regression on Age')
plt.yticks([0,1], ['Not Churned', 'Churned'])
plt.legend()
plt.show()

from google.colab import files
import pandas as pd

# Upload file
uploaded = files.upload()

# Get filename
filename = list(uploaded.keys())[0]

# Load into pandas
df = pd.read_csv(filename)
print(df.head())

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
import matplotlib.pyplot as plt
import numpy as np

# Features and target
X = df[['age']].values  # 2D array
y = df['is_churned'].values

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Scale features (important for SVM)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize SVM (linear kernel for simplicity)
svm_model = SVC(kernel='linear', class_weight='balanced', random_state=42)
svm_model.fit(X_train_scaled, y_train)

# Predict
y_pred = svm_model.predict(X_test_scaled)

# Evaluate
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# Optional: visualize decision boundary
plt.figure(figsize=(8,6))
# Scatter plot of age vs churn
y_jitter = y_test + np.random.uniform(-0.05, 0.05, size=y_test.shape)
plt.scatter(X_test_scaled, y_jitter, c=y_test, cmap='bwr', alpha=0.6)

# Plot decision boundary
x_vals = np.linspace(X_test_scaled.min()-1, X_test_scaled.max()+1, 100)
w = svm_model.coef_[0]
b = svm_model.intercept_[0]
y_decision = -(w[0]*x_vals + b)/1  # since only one feature
plt.plot(x_vals, y_decision, 'k--', label='Decision boundary')

plt.xlabel('Age (scaled)')
plt.ylabel('Is Churned (jittered)')
plt.title('SVM Decision Boundary on Age')
plt.yticks([0,1], ['Not Churned', 'Churned'])
plt.legend()
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
import matplotlib.pyplot as plt
import numpy as np

# Features and target
X = df[['age']].values  # 2D array
y = df['is_churned'].values

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize SVM with sigmoid kernel
svm_sigmoid = SVC(kernel='sigmoid', class_weight='balanced', random_state=42, probability=True)
svm_sigmoid.fit(X_train_scaled, y_train)

# Predict
y_pred = svm_sigmoid.predict(X_test_scaled)

# Evaluate
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# Visualize predicted probabilities / decision function
plt.figure(figsize=(8,6))
# Scatter plot of actual values
y_jitter = y_test + np.random.uniform(-0.05, 0.05, size=y_test.shape)
plt.scatter(X_test_scaled, y_jitter, c=y_test, cmap='bwr', alpha=0.6)

# Decision function curve
x_vals = np.linspace(X_test_scaled.min()-1, X_test_scaled.max()+1, 300).reshape(-1,1)
y_prob = svm_sigmoid.predict_proba(x_vals)[:,1]  # probability of class 1
plt.plot(x_vals, y_prob, 'k--', label='Predicted Probability (sigmoid kernel)')

plt.xlabel('Age (scaled)')
plt.ylabel('Is Churned')
plt.title('SVM with Sigmoid Kernel on Age')
plt.yticks([0,1], ['Not Churned', 'Churned'])
plt.legend()
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
import matplotlib.pyplot as plt
import numpy as np

# Features and target
X = df[['age']].values  # 2D array
y = df['is_churned'].values

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize SVM with polynomial kernel
svm_poly = SVC(kernel='poly', degree=3, class_weight='balanced', random_state=42, probability=True)
svm_poly.fit(X_train_scaled, y_train)

# Predict
y_pred = svm_poly.predict(X_test_scaled)

# Evaluate
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# Visualize predicted probabilities / decision function
plt.figure(figsize=(8,6))
# Scatter plot of actual values
y_jitter = y_test + np.random.uniform(-0.05, 0.05, size=y_test.shape)
plt.scatter(X_test_scaled, y_jitter, c=y_test, cmap='bwr', alpha=0.6)

# Decision function curve
x_vals = np.linspace(X_test_scaled.min()-1, X_test_scaled.max()+1, 300).reshape(-1,1)
y_prob = svm_poly.predict_proba(x_vals)[:,1]  # probability of class 1
plt.plot(x_vals, y_prob, 'k--', label='Predicted Probability (poly kernel)')

plt.xlabel('Age (scaled)')
plt.ylabel('Is Churned')
plt.title('SVM with Polynomial Kernel on Age')
plt.yticks([0,1], ['Not Churned', 'Churned'])
plt.legend()
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
import matplotlib.pyplot as plt
import numpy as np

# Features and target
X = df[['age']].values  # 2D array
y = df['is_churned'].values

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize SVM with RBF kernel
svm_rbf = SVC(kernel='rbf', class_weight='balanced', random_state=42, probability=True)
svm_rbf.fit(X_train_scaled, y_train)

# Predict
y_pred = svm_rbf.predict(X_test_scaled)

# Evaluate
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# Visualize predicted probabilities
plt.figure(figsize=(8,6))
# Scatter plot of actual values
y_jitter = y_test + np.random.uniform(-0.05, 0.05, size=y_test.shape)
plt.scatter(X_test_scaled, y_jitter, c=y_test, cmap='bwr', alpha=0.6)

# Decision function curve (probability)
x_vals = np.linspace(X_test_scaled.min()-1, X_test_scaled.max()+1, 300).reshape(-1,1)
y_prob = svm_rbf.predict_proba(x_vals)[:,1]  # probability of class 1
plt.plot(x_vals, y_prob, 'k--', label='Predicted Probability (RBF kernel)')

plt.xlabel('Age (scaled)')
plt.ylabel('Is Churned')
plt.title('SVM with RBF Kernel on Age')
plt.yticks([0,1], ['Not Churned', 'Churned'])
plt.legend()
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
import matplotlib.pyplot as plt
import numpy as np

# Features and target
X = df[['listening_time']].values  # 2D array
y = df['is_churned'].values

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize SVM with sigmoid kernel
svm_sigmoid = SVC(kernel='sigmoid', class_weight='balanced', random_state=42, probability=True)
svm_sigmoid.fit(X_train_scaled, y_train)

# Predict
y_pred = svm_sigmoid.predict(X_test_scaled)

# Evaluate
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# Visualize predicted probabilities / decision function
plt.figure(figsize=(8,6))
# Scatter plot of actual values
y_jitter = y_test + np.random.uniform(-0.05, 0.05, size=y_test.shape)
plt.scatter(X_test_scaled, y_jitter, c=y_test, cmap='bwr', alpha=0.6)

# Decision function curve
x_vals = np.linspace(X_test_scaled.min()-1, X_test_scaled.max()+1, 300).reshape(-1,1)
y_prob = svm_sigmoid.predict_proba(x_vals)[:,1]  # probability of class 1
plt.plot(x_vals, y_prob, 'k--', label='Predicted Probability (sigmoid kernel)')

plt.xlabel('Age (scaled)')
plt.ylabel('Is Churned')
plt.title('SVM with Sigmoid Kernel on Age')
plt.yticks([0,1], ['Not Churned', 'Churned'])
plt.legend()
plt.show()

# ============================
# Churn Prediction with Decision Tree
# ============================

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import classification_report, accuracy_score
import matplotlib.pyplot as plt



# Columns: ['age', 'listening_time', 'songs_played_per_day', 'skip_rate', 'is_churned']

# ----------------------------
# 2. Define features (X) and target (y)
# ----------------------------
X = df[['age', 'listening_time', 'songs_played_per_day', 'skip_rate']]
y = df['is_churned']   # 0 = Not Churned, 1 = Churned

# ----------------------------
# 3. Train/Test Split
# ----------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# ----------------------------
# 4. Train Decision Tree
# ----------------------------
clf = DecisionTreeClassifier(
    criterion="gini",  # or "entropy"
    max_depth=4,       # you can tune this
    random_state=42
)
clf.fit(X_train, y_train)

# ----------------------------
# 5. Predictions
# ----------------------------
y_pred = clf.predict(X_test)

print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# ----------------------------
# 6. Visualize Decision Tree
# ----------------------------
plt.figure(figsize=(12,8))
plot_tree(clf,
          feature_names=X.columns,
          class_names=["Not Churned", "Churned"],
          filled=True,
          rounded=True)
plt.show()

# ============================
# Churn Prediction with Decision Tree (Balanced)
# ============================

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import classification_report, accuracy_score
import matplotlib.pyplot as plt

# Columns: ['age', 'listening_time', 'songs_played_per_day', 'skip_rate', 'is_churned']

# ----------------------------
# 2. Define features (X) and target (y)
# ----------------------------
X = df[['age', 'listening_time', 'songs_played_per_day', 'skip_rate']]
y = df['is_churned']   # 0 = Not Churned, 1 = Churned

# ----------------------------
# 3. Train/Test Split
# ----------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# ----------------------------
# 4. Train Decision Tree with class weight balancing
# ----------------------------
clf = DecisionTreeClassifier(
    criterion="gini",      # or "entropy"
    max_depth=4,           # tuneable
    class_weight='balanced',  # handle imbalance
    random_state=42
)
clf.fit(X_train, y_train)

# ----------------------------
# 5. Predictions
# ----------------------------
y_pred = clf.predict(X_test)

print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# ----------------------------
# 6. Visualize Decision Tree
# ----------------------------
plt.figure(figsize=(12,8))
plot_tree(clf,
          feature_names=X.columns,
          class_names=["Not Churned", "Churned"],
          filled=True,
          rounded=True)
plt.show()

# ============================
# Churn Prediction with Decision Tree (Balanced)
# ============================

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import classification_report, accuracy_score
import matplotlib.pyplot as plt

# Columns: ['age', 'listening_time', 'songs_played_per_day', 'skip_rate', 'is_churned']

# ----------------------------
# 2. Define features (X) and target (y)
# ----------------------------
X = df[['age', 'listening_time', 'songs_played_per_day', 'skip_rate']]
y = df['is_churned']   # 0 = Not Churned, 1 = Churned

# ----------------------------
# 3. Train/Test Split
# ----------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# ----------------------------
# 4. Train Decision Tree with class weight balancing
# ----------------------------
clf = DecisionTreeClassifier(
    criterion="entropy",      # or "entropy"
    max_depth=4,           # tuneable
    class_weight='balanced',  # handle imbalance
    random_state=42
)
clf.fit(X_train, y_train)

# ----------------------------
# 5. Predictions
# ----------------------------
y_pred = clf.predict(X_test)

print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# ----------------------------
# 6. Visualize Decision Tree
# ----------------------------
plt.figure(figsize=(12,8))
plot_tree(clf,
          feature_names=X.columns,
          class_names=["Not Churned", "Churned"],
          filled=True,
          rounded=True)
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import classification_report, accuracy_score
import matplotlib.pyplot as plt

# ----------------------------
# 1. Features and target
# ----------------------------
# Use all columns except the target
X = df.drop(columns=['is_churned'])
X = X.drop(columns=['user_id'])
y = df['is_churned']

# ----------------------------
# 2. Train/Test Split
# ----------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# ----------------------------
# 3. Train Decision Tree with class weight balancing
# ----------------------------
clf = DecisionTreeClassifier(
    criterion='gini',       # or "entropy"
    max_depth=6,            # tuneable to avoid overfitting
    class_weight='balanced',  # handle imbalance
    random_state=42
)
clf.fit(X_train, y_train)

# ----------------------------
# 4. Predictions
# ----------------------------
y_pred = clf.predict(X_test)

# ----------------------------
# 5. Evaluation
# ----------------------------
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nConfusion Matrix:\n", pd.DataFrame(
    confusion_matrix(y_test, y_pred),
    index=['Actual Not Churned', 'Actual Churned'],
    columns=['Pred Not Churned', 'Pred Churned']
))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# ----------------------------
# 6. Visualize Decision Tree
# ----------------------------
plt.figure(figsize=(20,12))
plot_tree(clf,
          feature_names=X.columns,
          class_names=["Not Churned", "Churned"],
          filled=True,
          rounded=True,
          fontsize=10)
plt.show()

# ----------------------------
# 7. Feature Importance
# ----------------------------
feature_importance = pd.DataFrame({
    'Feature': X.columns,
    'Importance': clf.feature_importances_
}).sort_values(by='Importance', ascending=False)

print("\nFeature Importance:\n", feature_importance)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import matplotlib.pyplot as plt

# ----------------------------
# 1. Features and target
# ----------------------------
X = df[['songs_played_per_day', 'listening_time']]
y = df['is_churned']

# ----------------------------
# 2. Train/Test Split
# ----------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# ----------------------------
# 3. Train Decision Tree with class weight balancing
# ----------------------------
clf = DecisionTreeClassifier(
    criterion='gini',       # or "entropy"
    max_depth=4,            # tuneable to avoid overfitting
    class_weight='balanced',  # handle imbalance
    random_state=42
)
clf.fit(X_train, y_train)

# ----------------------------
# 4. Predictions
# ----------------------------
y_pred = clf.predict(X_test)

# ----------------------------
# 5. Evaluation
# ----------------------------
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nConfusion Matrix:\n", pd.DataFrame(
    confusion_matrix(y_test, y_pred),
    index=['Actual Not Churned', 'Actual Churned'],
    columns=['Pred Not Churned', 'Pred Churned']
))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# ----------------------------
# 6. Visualize Decision Tree
# ----------------------------
plt.figure(figsize=(12,8))
plot_tree(clf,
          feature_names=X.columns,
          class_names=["Not Churned", "Churned"],
          filled=True,
          rounded=True,
          fontsize=10)
plt.show()

# ----------------------------
# 7. Feature Importance
# ----------------------------
feature_importance = pd.DataFrame({
    'Feature': X.columns,
    'Importance': clf.feature_importances_
}).sort_values(by='Importance', ascending=False)

print("\nFeature Importance:\n", feature_importance)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import matplotlib.pyplot as plt

# ----------------------------
# 1. Features and target
# ----------------------------
X = df[['songs_played_per_day', 'listening_time']]
y = df['is_churned']

# ----------------------------
# 2. Train/Test Split
# ----------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# ----------------------------
# 3. Train Decision Tree with class weight balancing
# ----------------------------
clf = DecisionTreeClassifier(
    criterion='entropy',       # or "entropy"
    max_depth=4,            # tuneable to avoid overfitting
    class_weight='balanced',  # handle imbalance
    random_state=42
)
clf.fit(X_train, y_train)

# ----------------------------
# 4. Predictions
# ----------------------------
y_pred = clf.predict(X_test)

# ----------------------------
# 5. Evaluation
# ----------------------------
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nConfusion Matrix:\n", pd.DataFrame(
    confusion_matrix(y_test, y_pred),
    index=['Actual Not Churned', 'Actual Churned'],
    columns=['Pred Not Churned', 'Pred Churned']
))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# ----------------------------
# 6. Visualize Decision Tree
# ----------------------------
plt.figure(figsize=(12,8))
plot_tree(clf,
          feature_names=X.columns,
          class_names=["Not Churned", "Churned"],
          filled=True,
          rounded=True,
          fontsize=10)
plt.show()

# ----------------------------
# 7. Feature Importance
# ----------------------------
feature_importance = pd.DataFrame({
    'Feature': X.columns,
    'Importance': clf.feature_importances_
}).sort_values(by='Importance', ascending=False)

print("\nFeature Importance:\n", feature_importance)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import matplotlib.pyplot as plt
from imblearn.over_sampling import SMOTE  # pip install imbalanced-learn

# ----------------------------
# 1. Features and target
# ----------------------------
X = df[['songs_played_per_day', 'listening_time']]
y = df['is_churned']

# ----------------------------
# 2. Train/Test Split
# ----------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# ----------------------------
# 3. Oversample minority class using SMOTE
# ----------------------------
smote = SMOTE(random_state=42)
X_train_res, y_train_res = smote.fit_resample(X_train, y_train)

print("Original class distribution:\n", y_train.value_counts())
print("\nResampled class distribution:\n", y_train_res.value_counts())

# ----------------------------
# 4. Train Decision Tree
# ----------------------------
clf = DecisionTreeClassifier(
    criterion='gini',
    max_depth=4,
    random_state=42
)
clf.fit(X_train_res, y_train_res)

# ----------------------------
# 5. Predictions
# ----------------------------
y_pred = clf.predict(X_test)

# ----------------------------
# 6. Evaluation
# ----------------------------
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nConfusion Matrix:\n", pd.DataFrame(
    confusion_matrix(y_test, y_pred),
    index=['Actual Not Churned', 'Actual Churned'],
    columns=['Pred Not Churned', 'Pred Churned']
))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# ----------------------------
# 7. Visualize Decision Tree
# ----------------------------
plt.figure(figsize=(12,8))
plot_tree(clf,
          feature_names=X.columns,
          class_names=["Not Churned", "Churned"],
          filled=True,
          rounded=True,
          fontsize=10)
plt.show()

# ----------------------------
# 8. Feature Importance
# ----------------------------
feature_importance = pd.DataFrame({
    'Feature': X.columns,
    'Importance': clf.feature_importances_
}).sort_values(by='Importance', ascending=False)

print("\nFeature Importance:\n", feature_importance)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import numpy as np

# ----------------------------
# 1. Features and target
# ----------------------------
# List of features
features = [
    'songs_played_per_day', 'listening_time', 'age', 'skip_rate'
]

X = df[features]
y = df['is_churned']

# ----------------------------
# 2. Train/Test Split
# ----------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# ----------------------------
# 3. Feature Scaling
# ----------------------------
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ----------------------------
# 4. Initialize and Train MLP
# ----------------------------
mlp = MLPClassifier(
    hidden_layer_sizes=(64,32),  # 2 hidden layers with 32 and 16 neurons
    activation='relu',
    solver='adam',
    max_iter=500,
    random_state=42,

)

mlp.fit(X_train_scaled, y_train)

# ----------------------------
# 5. Predictions
# ----------------------------
y_pred = mlp.predict(X_test_scaled)

# ----------------------------
# 6. Evaluation
# ----------------------------
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from imblearn.over_sampling import SMOTE  # Optional: handle imbalance

# ----------------------------
# 1. Features and target
# ----------------------------
features = ['songs_played_per_day', 'listening_time', 'age', 'skip_rate']
X = df[features]
y = df['is_churned']

# ----------------------------
# 2. Train/Test Split
# ----------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# ----------------------------
# 3. Feature Scaling
# ----------------------------
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ----------------------------
# 4. Handle Class Imbalance (Optional but recommended)
# ----------------------------
smote = SMOTE(random_state=42)
X_train_res, y_train_res = smote.fit_resample(X_train_scaled, y_train)

# ----------------------------
# 5. Initialize and Train MLP with larger hidden layers
# ----------------------------
mlp = MLPClassifier(
    hidden_layer_sizes=(128, 64, 32),  # 3 hidden layers
    activation='relu',
    solver='adam',
    max_iter=1000,  # increase iterations for convergence
    random_state=42
)

mlp.fit(X_train_res, y_train_res)

# ----------------------------
# 6. Predictions
# ----------------------------
y_pred = mlp.predict(X_test_scaled)

# ----------------------------
# 7. Evaluation
# ----------------------------
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from imblearn.over_sampling import SMOTE  # Optional: handle imbalance

# ----------------------------
# 1. Features and target
# ----------------------------
features = ['songs_played_per_day', 'listening_time', 'age', 'skip_rate']
X = df[features]
y = df['is_churned']

# ----------------------------
# 2. Train/Test Split
# ----------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# ----------------------------
# 3. Feature Scaling
# ----------------------------
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ----------------------------
# 4. Handle Class Imbalance (Optional but recommended)
# ----------------------------
smote = SMOTE(random_state=42)
X_train_res, y_train_res = smote.fit_resample(X_train_scaled, y_train)

# ----------------------------
# 5. Initialize and Train MLP with larger hidden layers
# ----------------------------
mlp = MLPClassifier(
    hidden_layer_sizes=(128, 64, 64 ,32),  # 3 hidden layers
    activation='relu',
    solver='adam',
    max_iter=1000,  # increase iterations for convergence
    random_state=42
)

mlp.fit(X_train_res, y_train_res)

# ----------------------------
# 6. Predictions
# ----------------------------
y_pred = mlp.predict(X_test_scaled)

# ----------------------------
# 7. Evaluation
# ----------------------------
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

solvers = ['adam', 'sgd', 'lbfgs']

for solver in solvers:
    print(f"\n--- Training MLP with solver = {solver} ---")
    mlp = MLPClassifier(
        hidden_layer_sizes=(128, 64, 64, 32),
        activation='relu',
        solver=solver,
        max_iter=1000,
        random_state=42
    )

    mlp.fit(X_train_res, y_train_res)
    y_pred = mlp.predict(X_test_scaled)

    print("Accuracy:", accuracy_score(y_test, y_pred))
    print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
    print("Classification Report:\n", classification_report(y_test, y_pred))

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import xgboost as xgb
from imblearn.over_sampling import SMOTE  # optional for imbalance handling

# ----------------------------
# 1. Features and target
# ----------------------------
features = ['songs_played_per_day', 'listening_time', 'age', 'skip_rate']
X = df[features]
y = df['is_churned']

# ----------------------------
# 2. Train/Test Split
# ----------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# ----------------------------
# 3. Feature Scaling (optional for XGBoost, not strictly needed)
# ----------------------------
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ----------------------------
# 4. Handle Class Imbalance (optional but recommended)
# ----------------------------
smote = SMOTE(random_state=42)
X_train_res, y_train_res = smote.fit_resample(X_train_scaled, y_train)

# ----------------------------
# 5. Initialize and Train XGBoost Classifier
# ----------------------------
# Calculate scale_pos_weight = #negative / #positive
scale_pos_weight = (y_train_res == 0).sum() / (y_train_res == 1).sum()

xgb_model = xgb.XGBClassifier(
    n_estimators=200,
    max_depth=6,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    scale_pos_weight=scale_pos_weight,
    random_state=42,
    use_label_encoder=False,
    eval_metric='logloss'
)

xgb_model.fit(X_train_res, y_train_res)

# ----------------------------
# 6. Predictions
# ----------------------------
y_pred = xgb_model.predict(X_test_scaled)

# ----------------------------
# 7. Evaluation
# ----------------------------
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import xgboost as xgb
from imblearn.over_sampling import SMOTE

# ----------------------------
# 1. Features and target
# ----------------------------
features = ['songs_played_per_day', 'listening_time', 'age', 'skip_rate']
X = df[features]
y = df['is_churned']

# ----------------------------
# 2. Train/Test Split
# ----------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# ----------------------------
# 3. Feature Scaling (optional for XGBoost)
# ----------------------------
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ----------------------------
# 4. Handle Class Imbalance with SMOTE
# ----------------------------
smote = SMOTE(random_state=42)
X_train_res, y_train_res = smote.fit_resample(X_train_scaled, y_train)

# ----------------------------
# 5. Hyperparameter Tuning using GridSearchCV
# ----------------------------
scale_pos_weight = (y_train_res == 0).sum() / (y_train_res == 1).sum()

xgb_clf = xgb.XGBClassifier(
    objective='binary:logistic',
    random_state=42,
    use_label_encoder=False,
    eval_metric='logloss',
    scale_pos_weight=scale_pos_weight
)

param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.05, 0.1],
    'subsample': [0.7, 0.8, 1.0],
    'colsample_bytree': [0.7, 0.8, 1.0]
}

grid_search = GridSearchCV(
    estimator=xgb_clf,
    param_grid=param_grid,
    scoring='f1',  # focus on minority class
    cv=3,
    verbose=1,
    n_jobs=-1
)

grid_search.fit(X_train_res, y_train_res)

print("Best Parameters:", grid_search.best_params_)

# ----------------------------
# 6. Predictions
# ----------------------------
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test_scaled)

# ----------------------------
# 7. Evaluation
# ----------------------------
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))